{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# from sklearn.metrics import jaccard_score # scikit 0.21\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X shoudl be a numpy matrix, very likely sparse matrix: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix\n",
    "# T1 > T2 for overlapping clusters\n",
    "# T1 = Distance to centroid point to not include in other clusters\n",
    "# T2 = Distance to centroid point to include in cluster\n",
    "# T1 > T2 for overlapping clusters\n",
    "# T1 < T2 will have points which reside in no clusters\n",
    "# T1 == T2 will cause all points to reside in mutually exclusive clusters\n",
    "# Distance metric can be any from here: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html\n",
    "# filemap may be a list of point names in their order in X. If included, row numbers from X will be replaced with names from filemap. \n",
    " \n",
    "def canopy(X, T1, T2, distance_metric='euclidean', filemap=None):\n",
    "    canopies = dict()\n",
    "    X1_dist = pairwise_distances(X, metric=distance_metric)\n",
    "    canopy_points = set(range(X.shape[0]))\n",
    "    while canopy_points:\n",
    "        point = canopy_points.pop()\n",
    "        i = len(canopies)\n",
    "        canopies[i] = {\"c\":point, \"points\": list(np.where(X1_dist[point] < T2)[0])}\n",
    "        canopy_points = canopy_points.difference(set(np.where(X1_dist[point] < T1)[0]))\n",
    "    if filemap:\n",
    "        for canopy_id in canopies.keys():\n",
    "            canopy = canopies.pop(canopy_id)\n",
    "            canopy2 = {\"c\":filemap[canopy['c']], \"points\":list()}\n",
    "            for point in canopy['points']:\n",
    "                canopy2[\"points\"].append(filemap[point])\n",
    "            canopies[canopy_id] = canopy2\n",
    "    return canopies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(y_true, y_pred):\n",
    "    y_true = y_true.map({\"D1\": 0, \"D2\": 1, \"D3\": 2, \"D4\": 3,\"b\":0, \"g\":1, \"1\": 0, \"2\": 1, \"3\": 2, \"Iris-setosa\": 0, \"Iris-versicolor\": 1, \"Iris-virginica\": 2})\n",
    "\n",
    "#     print(jaccard_score(y_true, y_pred, average=\"weighted\"))    \n",
    "    print(adjusted_rand_score(y_true, y_pred))\n",
    "    print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDistance(vector1, vector2):\n",
    "        #print(vector1)\n",
    "        #print(vector2)\n",
    "        return np.sqrt(np.sum(np.power(vector1-vector2, 2)))\n",
    "\n",
    "def getDistance(row_center, row_sample):\n",
    "        #print(row_center)\n",
    "        row_center = np.asarray(row_center)\n",
    "        #row_center = np.asarray(row_sample)\n",
    "        return euclideanDistance(row_center, row_sample)\n",
    "\n",
    "def getSquaredError(data, kmeans, k):\n",
    "    distances = []\n",
    "    for i in range(k): # Qtd de clusters\n",
    "        distance = 0\n",
    "        for index_labels, value_labels in enumerate(kmeans.labels_): #kmeans.labels_ possui o cluster de cada elemento\n",
    "            if (i == value_labels):\n",
    "                #print(value_labels)\n",
    "                distance = distance + getDistance(kmeans.cluster_centers_[value_labels], data[index_labels])\n",
    "        distances.append(distance) #Erro quadratico medio de cada cluster\n",
    "    distances = np.asarray(distances)\n",
    "    error = np.sum(distances)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"soybean-small\", \"iris\", \"wine\", \"segmentation\", \"ionosphere\"]\n",
    "ks = [4,3,3,7,2]\n",
    "kmeansTypes = [\"random\", \"k-means++\", \"densityCanopy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = files[0]\n",
    "k = ks[0]\n",
    "\n",
    "data = pd.read_csv(\"datasets/\"+file+\".data\", header=None)\n",
    "if file == \"segmentation\" or file == \"wine\": #Target eh na primeira coluna\n",
    "    target = data.iloc[:,0]\n",
    "    data = data.iloc[:,1:]\n",
    "else: #Target na Ãºltima coluna\n",
    "    target = data.iloc[:,-1]\n",
    "    data = data.iloc[:,:-1]\n",
    "\n",
    "data = data.values\n",
    "distance_matrix = pairwise_distances(data, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    # demo_data = np.random.random_sample((2000, 5))\n",
    "    # print(demo_data.shape)\n",
    "#     data = auxData\n",
    "#     demo_data = data.values\n",
    "    clustering_time = []\n",
    "    clustering_error = []\n",
    "\n",
    "    for kmeansType in kmeansTypes:\n",
    "        start = time()\n",
    "        if kmeansType == \"densityCanopy\":\n",
    "            centers = np.asarray(densityCanopy())\n",
    "            l = len(centers)\n",
    "            kmeans = KMeans(n_clusters=l, random_state=100, init=centers, n_init=1, max_iter=100).fit(data)\n",
    "        else:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=100, init=kmeansType, n_init=1, max_iter=100).fit(data)\n",
    "        \n",
    "        getMetrics(target, kmeans.labels_)\n",
    "\n",
    "        error = getSquaredError(data, kmeans, k)\n",
    "        clustering_error.append(error)\n",
    "        #print(\"Erro:\", error)\n",
    "\n",
    "        end = time()\n",
    "        clustering_time.append(end - start)\n",
    "\n",
    "    print (\"\\n---Tempos---\")\n",
    "    print (\"random: \", clustering_time[0], \" kmeans++: \", clustering_time[1], \" densityCanopy: \", clustering_time[2])\n",
    "    \n",
    "    print (\"\\n---Erros---\")\n",
    "    print (\"random: \", clustering_error[0], \"kmeans++: \", clustering_error[1], \" densityCanopy: \", clustering_error[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Tempos---\n",
      "random:  0.002682209014892578  kmeans++:  0.006242990493774414  densityCanopy:  0.004130125045776367\n",
      "\n",
      "---Erros---\n",
      "random:  100.72984563120583 kmeans++:  96.99203638192614  densityCanopy:  103.90193746707101\n",
      "0.6024288940964433\n",
      "0.2127659574468085\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition 1\n",
    "def get_mean_distance(distance_matrix):\n",
    "    n = len(distance_matrix)\n",
    "    return distance_matrix.sum() / (n* (n-1)) if n > 1 else distance_matrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition 2\n",
    "def get_density(distance_matrix, mean_distance):\n",
    "    densities = np.zeros(len(distance_matrix))\n",
    "    \n",
    "    matrix = distance_matrix - mean_distance\n",
    "    matrix[matrix >= 0] = 0\n",
    "    matrix[matrix < 0] = 1\n",
    "\n",
    "    for i, row_i in enumerate(matrix):\n",
    "        densities[i] = row_i.sum()\n",
    "    return densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition 3\n",
    "def cluster_tightness(distance_matrix, mean_distance):\n",
    "    tightness = np.zeros(len(distance_matrix))\n",
    "    \n",
    "    for i, row_i in enumerate(distance_matrix):\n",
    "        mean = row_i[row_i < mean_distance].mean()\n",
    "        tightness[i] = 0 if mean == 0 else 1 / mean\n",
    "    return tightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition 4\n",
    "def clusters_dissimilarity(densities, distance_matrix):\n",
    "    n = len(distance_matrix)\n",
    "    dissimilarity = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        d_row = distance_matrix[i]\n",
    "        d_row = np.delete(d_row, np.where(d_row == 0))\n",
    "        dissimilarity[i] = d_row.min()\n",
    "    max_id = np.where(densities == densities.max())\n",
    "    dissimilarity[max_id] = distance_matrix[max_id].max()\n",
    "    return dissimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cluster(c, data, distance_matrix, mean_distance):\n",
    "    center_row = distance_matrix[c]\n",
    "    center_row[center_row < mean_distance] = 0\n",
    "    \n",
    "    #pontos a serem removidos\n",
    "    points = np.where(center_row == 0)\n",
    "    data = np.delete(data, points, axis=0)\n",
    "    \n",
    "    distance_matrix = np.delete(distance_matrix, points, axis=0)\n",
    "    distance_matrix = np.delete(distance_matrix, points, axis=1)\n",
    "    \n",
    "    return data, distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeOutliers(aux_D, densities, inv_a, s, meanDis):\n",
    "    #remove elemento com densidade = 1 e que o s[i] seja maior que o raio\n",
    "    outliers = []\n",
    "    for i, row_i in enumerate(aux_D.values):\n",
    "        if densities[i] == 1 and s[i] == max(s) and aux_D.shape[0] > 1:\n",
    "            outliers.append(i)\n",
    "    aux_D.drop(outliers, inplace=True) #removendo outliers\n",
    "    aux_D.reset_index(drop=True, inplace=True)\n",
    "    densities = np.delete(densities, outliers, 0)\n",
    "    inv_a = np.delete(inv_a, outliers, 0)\n",
    "    s = np.delete(s, outliers, 0)\n",
    "    return aux_D, densities, inv_a, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densityCanopy():\n",
    "    print (\"\\n----- \"+file+\" -----\")\n",
    "    centers = []\n",
    "    db = data.copy()\n",
    "    d_matrix = distance_matrix.copy()\n",
    "    \n",
    "    #Step 1\n",
    "    mean_distance = get_mean_distance(d_matrix)\n",
    "    density_set = get_density(d_matrix, mean_distance)\n",
    "    c = np.argmax(density_set)\n",
    "    centers.append(db[c])\n",
    "    db, d_matrix = remove_cluster(c, db, d_matrix, mean_distance)\n",
    "    \n",
    "    #Step 2\n",
    "    mean_distance = get_mean_distance(d_matrix)\n",
    "    density_set = get_density(d_matrix, mean_distance)\n",
    "    tightness_set = cluster_tightness(d_matrix, mean_distance)\n",
    "    dissimilarity_set = clusters_dissimilarity(density_set, d_matrix)\n",
    "    w_set = density_set * tightness_set * dissimilarity_set # Product Weight\n",
    "    c = np.argmax(w_set)\n",
    "    centers.append(db[c])\n",
    "    db, d_matrix = remove_cluster(c, db, d_matrix, mean_distance)\n",
    "    \n",
    "    #Step 3\n",
    "    while len(db) > 1:\n",
    "        \n",
    "        mean_distance = get_mean_distance(d_matrix)\n",
    "        density_set = get_density(d_matrix, mean_distance)\n",
    "        tightness_set = cluster_tightness(d_matrix, mean_distance)\n",
    "        \n",
    "        # Distancia das instancias para os centros jÃ¡ escolhidos\n",
    "        clusters_distance = pairwise_distances(db, centers, metric='euclidean')\n",
    "        \n",
    "        w_set = density_set * tightness_set\n",
    "        for i in range(len(db)):\n",
    "            w_set[i] = reduce(lambda x, y: x * y, clusters_distance[i] * w_set[i])\n",
    "        \n",
    "        c = np.argmax(w_set)\n",
    "        centers.append(db[c])\n",
    "        db, d_matrix = remove_cluster(c, db, d_matrix, mean_distance)\n",
    "        \n",
    "    if len(db) == 1:\n",
    "        centers.append(db[0])\n",
    "    \n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09386941  0.07403991]\n",
      " [-0.10084067  0.18505222]]\n"
     ]
    }
   ],
   "source": [
    "mu, sigma = 0, 0.1 \n",
    "# creating a noise with the same dimension as the dataset (2,2) \n",
    "noise = np.random.normal(mu, sigma, [2,2]) \n",
    "print(noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u A X = 0, u Y A = 0\n",
    "Ï x = Ï y = 2\n",
    "\n",
    "u B X = 5, u Y B = 1\n",
    "Ï x = 1 , Ï y = 2\n",
    "\n",
    "u C X = 5, u C Y = â2\n",
    "Ï x = Ï y = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
